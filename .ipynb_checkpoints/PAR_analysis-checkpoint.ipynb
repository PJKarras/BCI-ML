{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPORT NEEDED MODULES \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cmocean\n",
    "import cmocean.cm as cmo\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time as t\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Desktop\\Thermo_R_Code\\FinalThermo\\new\\BCI-ML\n"
     ]
    }
   ],
   "source": [
    "startingDir = os.getcwd()\n",
    "print(startingDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Takes in a string that represents name of csv file and \n",
    "# converts that string to one that matches the date string present\n",
    "# in the old master file (old master file refers to the raw excel file\n",
    "# filled with extraneous data from BCI)that contains the date/times \n",
    "# for recorded GPP values\n",
    "Description pulled from make-master-non-c0ontinuous\n",
    "\n",
    "# takes a file name in the form YYYY_MM_DD_HH_MM_bci_t and returns new date in form 2/17/2015 15:30. \n",
    "# with times not at 30 minute intervals rounded to the nearest 30 min\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# takes a file name in the form YYYY_MM_DD_HH_MM_bci_t and returns new date in form 2/17/2015 15:30. \n",
    "# with times not at 30 minute intervals rounded to the nearest 30 min\n",
    "def getRoundedTime(inTime):\n",
    "    minDivFiveFlag = True\n",
    "    year = inTime[2:4]\n",
    "    month = inTime[5:7]\n",
    "    day = inTime[8:10]\n",
    "    hour = inTime[11:13]\n",
    "    minuteInt = int(inTime[14:16])\n",
    "    # Round minute such that those 0-15 and 45-59 get rounded to 0 and all other values to 30\n",
    "    minuteStr = \"XX\"\n",
    "    if(minuteInt <= 15 or minuteInt >= 45):\n",
    "        minuteStr = \"00\"\n",
    "    else:\n",
    "        minuteStr = \"30\"\n",
    "    second = \"00\"\n",
    "    if(int(hour) == 0):\n",
    "        hour = \"0\"\n",
    "    elif(int(hour) < 10):\n",
    "        hour = hour[1:2]\n",
    "    # No leading zeroes in day or month\n",
    "    if(int(month) < 10):\n",
    "        month = month[1:2]\n",
    "    if(int(day) < 10):\n",
    "        day = day[1:2]\n",
    "    result = month + \"/\" + day + \"/\" + year + \" \" + hour + \":\" + minuteStr #+ \":\" + second + \" \" + timeDay\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Takes in a date in form 2/17/2015 15:30 and another date in the same form, checks if times are the same\"\"\"\n",
    "def timeMatch(inTimeOne, inTimeTwo):\n",
    "    timeOne = inTimeOne[inTimeOne.index(\" \"):]\n",
    "    timeTwo = inTimeTwo[inTimeTwo.index(\" \"):]\n",
    "    if(timeOne == timeTwo):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(timeMatch(\"9/30/2315 15:00\", \"9/30/15 15:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfilesMLLocation = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new2/csvdataFULL/BCIML_Dataset\"\n",
    "masterfileMLLocation = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new2/csvdataFULL/BCIML_Dataset/populatedGPPsFULL.csv\"\n",
    "testData = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new/BCI-ML/Monthly_Data_Summaries_csv/DAYTIME_new9930834___/test_data_DAYTIME_13-446994955314361.csv\"\n",
    "analysisLocation = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new2/tempplots\"\n",
    "all_working = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new2/all-30min-Apr2018_working.csv\"\n",
    "\n",
    "# Get data from test data, where we store targets and predictions of network\n",
    "testDataInfo = pd.read_csv(testData)\n",
    "testDataFileNames = testDataInfo['filename']\n",
    "allInfo = pd.read_csv(all_working)\n",
    "par_vals = allInfo['PAR']\n",
    "date_vals = allInfo['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PAR           date\n",
      "0     2043.833333  2/17/15 13:00\n",
      "1     1013.350000  2/17/15 13:30\n",
      "2      804.966667  2/17/15 14:00\n",
      "3      658.200000  2/17/15 14:30\n",
      "4      424.350000  2/17/15 15:00\n",
      "...           ...            ...\n",
      "8363     0.340000  9/30/15 21:30\n",
      "8364     0.340000  9/30/15 22:00\n",
      "8365     0.340167  9/30/15 22:30\n",
      "8366     0.340000  9/30/15 23:00\n",
      "8367     0.340000  9/30/15 23:30\n",
      "\n",
      "[8368 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Each data frame holds PAR and corresponding date values, each split into individual months\n",
    "#\n",
    "par_date_dic = {\"PAR\": par_vals, \"date\": date_vals}\n",
    "full_df = pd.concat(par_date_dic,axis=1)\n",
    "regex_time_range = re.compile(\"2015_.._.._(0[4-9]_..|1[0-9]_..|20_00)_bci_t\")\n",
    "\n",
    "# Filter out time space we want ####################################\n",
    "#with contains and regex abo e\n",
    "\n",
    "print(full_df)\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/27975069/how-to-filter-rows-containing-a-string-pattern-from-a-pandas-dataframe\n",
    "feb_df = full_df[full_df['date'].str.startswith('2/')]\n",
    "mar_df = full_df[full_df['date'].str.startswith('3/')]\n",
    "apr_df = full_df[full_df['date'].str.startswith('4/')]\n",
    "may_df = full_df[full_df['date'].str.startswith('5/')]\n",
    "jun_df = full_df[full_df['date'].str.startswith('6/')]\n",
    "jul_df = full_df[full_df['date'].str.startswith('7/')]\n",
    "aug_df = full_df[full_df['date'].str.startswith('8/')]\n",
    "sep_df = full_df[full_df['date'].str.startswith('9/')]\n",
    "\n",
    "# save data frames\n",
    "par_csv_save_location = r\"C:/Users/Peter/Desktop/Thermo_R_Code/FinalThermo/new/BCI-ML/Images and Plots/PAR Data\"\n",
    "os.chdir(par_csv_save_location)\n",
    "feb_df.to_csv('feb_par.csv')\n",
    "mar_df.to_csv('mar_par.csv')\n",
    "apr_df.to_csv('apr_par.csv')\n",
    "may_df.to_csv('may_par.csv')\n",
    "jun_df.to_csv('jun_par.csv')\n",
    "jul_df.to_csv('jul_par.csv')\n",
    "aug_df.to_csv('aug_par.csv')\n",
    "sep_df.to_csv('sep_par.csv')\n",
    "full_df.to_csv('full_par.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "r = list()\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2/17/15 13:30'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.iloc[1]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erer\n",
      "newFilename    2/19/15 13:00\n",
      "PAR                  1361.35\n",
      "PARDIFF              528.283\n",
      "count                     13\n",
      "mean                 1889.63\n",
      "std                   377.66\n",
      "min                   854.05\n",
      "25%                  1908.17\n",
      "50%                  2043.83\n",
      "75%                   2093.5\n",
      "max                  2173.17\n",
      "dtype: object\n",
      "erer\n",
      "newFilename    2/19/15 12:00\n",
      "PAR                  484.417\n",
      "PARDIFF              1339.53\n",
      "count                     13\n",
      "mean                 1823.95\n",
      "std                  568.784\n",
      "min                  484.417\n",
      "25%                  1949.83\n",
      "50%                  2036.83\n",
      "75%                   2155.5\n",
      "max                   2188.5\n",
      "dtype: object\n",
      "ASDASDASD\n",
      "[newFilename    2/19/15 13:00\n",
      "PAR                  1361.35\n",
      "PARDIFF              528.283\n",
      "count                     13\n",
      "mean                 1889.63\n",
      "std                   377.66\n",
      "min                   854.05\n",
      "25%                  1908.17\n",
      "50%                  2043.83\n",
      "75%                   2093.5\n",
      "max                  2173.17\n",
      "dtype: object, newFilename    2/19/15 12:00\n",
      "PAR                  484.417\n",
      "PARDIFF              1339.53\n",
      "count                     13\n",
      "mean                 1823.95\n",
      "std                  568.784\n",
      "min                  484.417\n",
      "25%                  1949.83\n",
      "50%                  2036.83\n",
      "75%                   2155.5\n",
      "max                   2188.5\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Goes through a list of filenames, rounding their time values to the nearest 30 min interval,\n",
    "and then looking at PAR values from the mastermaster file (all-30min-Apr2018_working) for the \n",
    "same times but +/- 6 days apart (THIS CAN BE CHANGED, MARKED WITH #XKDCD.) \n",
    "The average, median, standard deviation and range is then pulled form these\n",
    "14 days of PAR data. Each of these values is saved, along with the exact time PAR value, and the \n",
    "difference between these two data points\"\"\"\n",
    "os.chdir(csvfilesMLLocation)\n",
    "sourceFile = \"populatedGPPsAFTERcopy.csv\"\n",
    "daytimePopulatedInfo =pd.read_csv(sourceFile)\n",
    "filenames = daytimePopulatedInfo[\"FILE\"]\n",
    "\n",
    "savedIndex = 0\n",
    "# Holds individual rows of table, will be concatted\n",
    "fullInfoList = list()\n",
    "newFilename = \"2/19/15 13:00\"\n",
    "for i, r in full_df.iloc[:1000].iterrows():\n",
    "    if(r['date']==newFilename):\n",
    "        statList = list()\n",
    "        backIdx = i-1\n",
    "        forwardIdx = i+1\n",
    "        statList.append(full_df.iloc[i][\"PAR\"])\n",
    "        #XCKD, can change range of days pulled from here by changing size of list\n",
    "        # where (size of list - 1) / 2 = number of days in each direction\n",
    "        while(len(statList) < 13):\n",
    "            # Start iterating backwards and forwards, adding PAR values to stat list as we go\n",
    "            # foward and backindex will stop if they reach endpoints, and the other will grow the list\n",
    "            # if both endpoints are reached, we stop.\n",
    "            if(backIdx > -1):\n",
    "                if(timeMatch(newFilename, full_df.iloc[backIdx]['date'])):\n",
    "                    statList.append(full_df.iloc[backIdx]['PAR'])\n",
    "            if(forwardIdx < len(full_df)):\n",
    "                if(timeMatch(newFilename, full_df.iloc[forwardIdx]['date'])):\n",
    "                    statList.append(full_df.iloc[forwardIdx]['PAR'])\n",
    "            backIdx = backIdx - 1\n",
    "            forwardIdx = forwardIdx + 1\n",
    "            if(forwardIdx >= len(full_df) and backIdx <= -1):\n",
    "                break\n",
    "        statSeries = pd.Series(statList)\n",
    "        print(\"erer\")\n",
    "        descSeries = statSeries.describe()\n",
    "        detailsSeries = pd.Series({'newFilename': newFilename,'PAR': full_df.iloc[i][\"PAR\"], 'PARDIFF':abs(full_df.iloc[i][\"PAR\"]-descSeries['mean'])})\n",
    "        finalInfoSeries = pd.concat([detailsSeries, descSeries])\n",
    "        print(finalInfoSeries)\n",
    "\n",
    "        fullInfoList.append(finalInfoSeries)\n",
    "        break\n",
    "        #IF you reachecd outside of above for loop, you missed break and \n",
    "        #current newFilename you are searching for does not have a PAR to get... will be printed out in a text log\n",
    "\n",
    "newFilename = \"2/19/15 12:00\"\n",
    "for i, r in full_df.iloc[:1000].iterrows():\n",
    "    if(r['date']==newFilename):\n",
    "        statList = list()\n",
    "        backIdx = i-1\n",
    "        forwardIdx = i+1\n",
    "        statList.append(full_df.iloc[i][\"PAR\"])\n",
    "        #XCKD, can change range of days pulled from here by changing size of list\n",
    "        # where (size of list - 1) / 2 = number of days in each direction\n",
    "        while(len(statList) < 13):\n",
    "            # Start iterating backwards and forwards, adding PAR values to stat list as we go\n",
    "            # foward and backindex will stop if they reach endpoints, and the other will grow the list\n",
    "            # if both endpoints are reached, we stop.\n",
    "            if(backIdx > -1):\n",
    "                if(timeMatch(newFilename, full_df.iloc[backIdx]['date'])):\n",
    "                    statList.append(full_df.iloc[backIdx]['PAR'])\n",
    "            if(forwardIdx < len(full_df)):\n",
    "                if(timeMatch(newFilename, full_df.iloc[forwardIdx]['date'])):\n",
    "                    statList.append(full_df.iloc[forwardIdx]['PAR'])\n",
    "            backIdx = backIdx - 1\n",
    "            forwardIdx = forwardIdx + 1\n",
    "            if(forwardIdx >= len(full_df) and backIdx <= -1):\n",
    "                break\n",
    "        statSeries = pd.Series(statList)\n",
    "        print(\"erer\")\n",
    "        descSeries = statSeries.describe()\n",
    "        detailsSeries = pd.Series({'newFilename': newFilename,'PAR': full_df.iloc[i][\"PAR\"], 'PARDIFF':abs(full_df.iloc[i][\"PAR\"]-descSeries['mean'])})\n",
    "        finalInfoSeries = pd.concat([detailsSeries, descSeries])\n",
    "        print(finalInfoSeries)\n",
    "\n",
    "        fullInfoList.append(finalInfoSeries)\n",
    "        break\n",
    "        #IF you reachecd outside of above for loop, you missed break and \n",
    "        #current newFilename you are searching for does not have a PAR to get... will be printed out in a text log  \n",
    "\n",
    "pd.concat(fullInfoList,join='inner')\n",
    "print(\"ASDASDASD\")\n",
    "print(fullInfoList)\n",
    "# FIGURE OUT HOW TO COMBINE THESE SERIESES SUCH THAT WE MAY PRINT THEM NICELY\n",
    "        \n",
    "#print(\"popop\",full_df.loc[1])\n",
    "\n",
    "# savedIndex = 0\n",
    "# for filename in filenames:\n",
    "#     newFilename = getRoundedTime(filename)\n",
    "#     for i, r in full_df.iloc[savedIndex:].iterrows():\n",
    "#         savedIndex = i\n",
    "#         # Go through each row where last left off, looking for matching time\n",
    "#         if(r['date']==newFilename):\n",
    "#             statList = list()\n",
    "#             backIdx = i-1\n",
    "#             forwardIdx = i+1\n",
    "#             statList.append(full_df.iloc[i][\"PAR\"])\n",
    "            \n",
    "#             #XCKD, can change range of days pulled from here by changing size of list\n",
    "#             # where (size of list - 1) / 2 = number of days in each direction\n",
    "#             while(len(statList) < 13):\n",
    "#                 # Start iterating backwards and forwards, adding PAR values to stat list as we go\n",
    "#                 # foward and backindex will stop if they reach endpoints, and the other will grow the list\n",
    "#                 # if both endpoints are reached, we stop.\n",
    "#                 if(backIdx > -1):\n",
    "#                     if(timeMatch(newFilename, full_df.iloc[backIdx]['date'])):\n",
    "#                         statList.append(full_df.iloc[backIdx]['PAR'])\n",
    "#                 if(forwardIdx < len(full_df)):\n",
    "#                     if(timeMatch(newFilename, full_df.iloc[forwardIdx]['date'])):\n",
    "#                         statList.append(full_df.iloc[forwardIdx]['PAR'])\n",
    "#                 backIdx = backIdx - 1\n",
    "#                 forwardIdx = forwardIdx + 1\n",
    "#                 if(not(forwardIdx < len(full_df) and backIdx > -1)):\n",
    "#                     break\n",
    "#             statSeries = pd.Series(statList)\n",
    "#             print(\"erer\")\n",
    "#             print(statSeries.describe())\n",
    "#             detailsSeries = pd.Series({'newFilename': newFilename,'PAR': full_df.iloc[i][\"PAR\"]})\n",
    "#             print(detailsSeries)\n",
    "#             finalInfoSeries = pd.concat([detailsSeries, statSeries.describe()])\n",
    "#             print(finalInfoSeries)\n",
    "#             break\n",
    "#             #IF you reachecd outside of above for loop, you missed break and \n",
    "#             #current newFilename you are searching for does not have a PAR to get... will be printed out in a text log \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/19/2015 13:00\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
